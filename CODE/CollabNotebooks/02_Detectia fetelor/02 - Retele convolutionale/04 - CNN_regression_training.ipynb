{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNE17h+LoGLqUv1gvR0f5c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sIlI5gAju7tn","executionInfo":{"status":"ok","timestamp":1745060882322,"user_tz":-180,"elapsed":5844,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}},"outputId":"2cea98f1-d255-4efa-d975-c38c23a54bed"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idiyCUw2qi0M","executionInfo":{"status":"ok","timestamp":1745060110175,"user_tz":-180,"elapsed":24862,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}},"outputId":"c18625d7-d3a8-45b9-9b73-63858ed7272a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["import google.colab\n","google.colab.drive.mount('/content/gdrive/', force_remount=True)"]},{"cell_type":"code","source":["import sys\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from scipy.ndimage import grey_dilation\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from google.colab import files\n","from IPython.display import display\n","import glob\n","import math\n","from torchinfo import summary"],"metadata":{"id":"Ql1EvV5zq1RF","executionInfo":{"status":"ok","timestamp":1745060938824,"user_tz":-180,"elapsed":6,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/gdrive/MyDrive/Facultate Informatica/Profesor/2024 - 2025/Biometrie/Curs/ColabMount/DATA/train_face_large_dataset'\n","model_dir = '/content/gdrive/MyDrive/Facultate Informatica/Profesor/2024 - 2025/Biometrie/Curs/ColabMount/Models/Face/Elementary_regression'"],"metadata":{"id":"H0S9u30vq3zm","executionInfo":{"status":"ok","timestamp":1745060129263,"user_tz":-180,"elapsed":8,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["images_dir = os.path.join(data_dir, 'images')\n","labels_dir = os.path.join(data_dir, 'labels')"],"metadata":{"id":"MH9zYaEtrTfA","executionInfo":{"status":"ok","timestamp":1745060131139,"user_tz":-180,"elapsed":12,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["if os.path.isdir(data_dir):\n","  print(\"Data directory exists\")\n","else:\n","  print(\"Data directory does not exist\")\n","if os.path.isdir(model_dir):\n","  print(\"Model directory exists\")\n","else:\n","  print(\"Model directory does not exist\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQnn4ovkrVf4","executionInfo":{"status":"ok","timestamp":1745060140464,"user_tz":-180,"elapsed":6876,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}},"outputId":"f7deb619-7fa7-4ec9-e95e-1e29d0e393bf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Data directory exists\n","Model directory exists\n"]}]},{"cell_type":"code","source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"aEjFmSIqrX8V","executionInfo":{"status":"ok","timestamp":1745060146039,"user_tz":-180,"elapsed":17,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])"],"metadata":{"id":"2wPCCktXrfJI","executionInfo":{"status":"ok","timestamp":1745060148396,"user_tz":-180,"elapsed":4,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    images, targets = zip(*batch)  # Separate images and labels\n","    images = torch.stack(images, 0)  # Stack images normally\n","    return images, list(targets)  # Return targets as a list (variable-length)"],"metadata":{"id":"v_fiXd7crfrQ","executionInfo":{"status":"ok","timestamp":1745060150190,"user_tz":-180,"elapsed":9,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class YOLODataset(Dataset):\n","    def __init__(self, img_dir, label_dir, transform=None):\n","        self.img_dir = img_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","        self.images = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")) + glob.glob(os.path.join(img_dir, \"*.png\")))\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        img_name = os.path.basename(img_path).split(\".\")[0]\n","        label_path = os.path.join(self.label_dir, f\"{img_name}.txt\")\n","\n","        # Load image with PIL to get its size\n","        # print(img_path)\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        # Store the original size of the image before any transformations\n","        original_width, original_height = image.size\n","\n","        # Apply transformations if available (such as resizing, etc.)\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # After the transformation, image is a tensor with shape [C, H, W], so:\n","        new_width, new_height = image.size(2), image.size(1)  # Access width and height from tensor\n","\n","        # Load labels (if exists)\n","        boxes = []\n","        if os.path.exists(label_path):\n","            with open(label_path, \"r\") as f:\n","                for line in f:\n","                    values = list(map(float, line.strip().split()))\n","                    # Rescale the bounding box coordinates\n","                    norm_x_center, norm_y_center, norm_bbox_width, norm_bbox_height = values[1], values[2], values[3], values[4]\n","\n","                    #recalculates the original pixel coordinates from norm\n","                    ox_center = norm_x_center * original_width\n","                    oy_center = norm_y_center * original_height\n","                    obbox_width = norm_bbox_width * original_width\n","                    obbox_height = norm_bbox_height * original_height\n","\n","\n","                    # Rescale bounding box coordinates to the new image size (after transformation)\n","                    x_center = ox_center * new_width / original_width\n","                    y_center = oy_center * new_height / original_height\n","                    width = obbox_width * new_width / original_width\n","                    height = obbox_height * new_height / original_height\n","\n","                    #renormalize the coordinates\n","\n","                    n_x_center = x_center/new_width\n","                    n_y_center = y_center/new_height\n","                    n_width = width/new_width\n","                    n_height = width/new_height\n","\n","                    # Append the adjusted box\n","                    boxes.append([values[0], n_x_center, n_y_center, n_width, n_height])\n","\n","        return image, torch.tensor(boxes)"],"metadata":{"id":"DQw6QC0arssX","executionInfo":{"status":"ok","timestamp":1745060151943,"user_tz":-180,"elapsed":1,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class YOLOFaceCNN(nn.Module):\n","    def __init__(self, grid_size=7):\n","        super(YOLOFaceCNN, self).__init__()\n","        self.grid_size = grid_size\n","\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3), nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 128, kernel_size=1), nn.LeakyReLU(0.1),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.LeakyReLU(0.1),\n","            nn.Conv2d(256, 256, kernel_size=1), nn.LeakyReLU(0.1),\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.LeakyReLU(0.1),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(512 * 14 * 14, 4096),\n","            nn.BatchNorm1d(4096),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, self.grid_size ** 2 * 5),  # Predicts (p, x, y, w, h) per grid cell\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        x = self.fc_layers(x)\n","        return x.view(-1, self.grid_size, self.grid_size, 5)  # Now predicts bounding boxes\n"],"metadata":{"id":"yuKcQRz6rwjl","executionInfo":{"status":"ok","timestamp":1745060155689,"user_tz":-180,"elapsed":12,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Funcția de cost"],"metadata":{"id":"9bjjR_r9xJ-l"}},{"cell_type":"markdown","source":["Această funcție de cost (YOLOFaceLoss) este concepută pentru a antrena un model de tip YOLO adaptat pentru detectarea fețelor într-o imagine, unde fiecare celulă a unei grile bidimensionale decide dacă există o față și estimează poziția și dimensiunile acesteia. Funcția ia în considerare atât celulele care conțin fețe, cât și cele care nu conțin, aplicând penalizări diferite în funcție de situație.\n","\n","Funcția primește ca intrare două tensori: pred (predicțiile modelului) și target (valorile reale), fiecare de dimensiune (batch, grid_size, grid_size, 5), unde cei cinci coeficienți reprezintă: probabilitatea prezenței unei fețe, coordonatele centrului (x, y) și dimensiunile (lățime, înălțime) ale feței în cadrul celulei."],"metadata":{"id":"IOYERmujxIYc"}},{"cell_type":"code","source":["class YOLOFaceLoss(nn.Module):\n","    def __init__(self, lambda_coord=2, lambda_noobj=0.02):\n","        super(YOLOFaceLoss, self).__init__()\n","        self.lambda_coord = lambda_coord  # Weight for face presence\n","        self.lambda_noobj = lambda_noobj  # Weight for no-face cells\n","        self.mse = nn.MSELoss(reduction='sum')  # Mean Squared Error loss (sum for all grid cells)\n","        self.bce = nn.BCELoss(reduction='sum')  # Binary Cross-Entropy Loss\n","\n","    def forward(self, pred, target):\n","        \"\"\"\n","        pred: (batch, grid_size, grid_size, 5) - Model predictions\n","        target: (batch, grid_size, grid_size, 5) - Ground truth (x, y, w, h, confidence)\n","        \"\"\"\n","        # Masks for face and no-face cells\n","        obj_mask = target[..., 0] > 0  # Cells where a face exists (probability is in the first position)\n","        no_obj_mask = target[..., 0] == 0  # Cells without a face (probability is in the first position)\n","\n","        # Coordinate Loss (x, y, w, h)\n","        coord_loss = 0.0\n","        if obj_mask.any():  # Only apply loss if there are faces\n","            coord_loss = self.lambda_coord * self.mse(pred[obj_mask][..., 1:5], target[obj_mask][..., 1:5])\n","\n","        # Confidence Score Loss (Object cells)\n","        obj_loss = 0.0\n","        if obj_mask.any():  # Only apply loss for cells containing faces\n","            obj_loss = self.bce(pred[obj_mask][..., 0], target[obj_mask][..., 0])\n","\n","        # Confidence Score Loss (No object cells)\n","        no_obj_loss = 0.0\n","        if no_obj_mask.any():  # Only apply loss for cells containing no faces\n","            no_obj_loss = self.lambda_noobj * self.bce(pred[no_obj_mask][..., 0], target[no_obj_mask][..., 0])\n","\n","        # Check for NaN values in loss terms\n","        if math.isnan(coord_loss) or math.isnan(obj_loss) or math.isnan(no_obj_loss):\n","            print(f\"NaN detected in loss! coord_loss: {coord_loss}, obj_loss: {obj_loss}, no_obj_loss: {no_obj_loss}\")\n","\n","        # Return the total loss\n","        # print(f\"coord_loss: {coord_loss}  obj_loss: {obj_loss} no_obj_loss: {no_obj_loss}\")\n","        total_loss = coord_loss + obj_loss + no_obj_loss\n","\n","        return total_loss, coord_loss, obj_loss, no_obj_loss"],"metadata":{"id":"eaojJvrcr3DT","executionInfo":{"status":"ok","timestamp":1745060161678,"user_tz":-180,"elapsed":39,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["hyperparameters = {\n","    \"learning_rate\": 0.0001,\n","    \"epochs\": 60,\n","    \"batch_size\": 16,\n","    \"grid_size\" : 7,\n","    \"lambda_coord\": 0.8,\n","    \"lambda_noobj\": 0.005,\n","    \"device\": str(DEVICE),\n","    \"model_name_root\": \"yolo_like_simple_loss_jp\"\n","}"],"metadata":{"id":"9AYrHTQ1sTVz","executionInfo":{"status":"ok","timestamp":1745060191289,"user_tz":-180,"elapsed":9,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_data = YOLODataset(img_dir=images_dir, label_dir=labels_dir, transform=transform)\n"],"metadata":{"id":"h4kea1APsZXG","executionInfo":{"status":"ok","timestamp":1745060283389,"user_tz":-180,"elapsed":16309,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_data, batch_size=hyperparameters['batch_size'], shuffle=True, collate_fn=collate_fn)\n","model = YOLOFaceCNN(grid_size=hyperparameters['grid_size']).to(DEVICE)"],"metadata":{"id":"mubA21i1stGZ","executionInfo":{"status":"ok","timestamp":1745060302589,"user_tz":-180,"elapsed":4686,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["summary(model, input_size=(1, 3, 224, 224))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jjyUvIcuyun","executionInfo":{"status":"ok","timestamp":1745061101022,"user_tz":-180,"elapsed":249,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}},"outputId":"11fc9cb6-4263-4f30-9dd9-84dd153980ea"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","YOLOFaceCNN                              [1, 7, 7, 5]              --\n","├─Sequential: 1-1                        [1, 512, 14, 14]          --\n","│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,472\n","│    └─LeakyReLU: 2-2                    [1, 64, 112, 112]         --\n","│    └─MaxPool2d: 2-3                    [1, 64, 56, 56]           --\n","│    └─Conv2d: 2-4                       [1, 192, 56, 56]          110,784\n","│    └─LeakyReLU: 2-5                    [1, 192, 56, 56]          --\n","│    └─MaxPool2d: 2-6                    [1, 192, 28, 28]          --\n","│    └─Conv2d: 2-7                       [1, 128, 28, 28]          24,704\n","│    └─LeakyReLU: 2-8                    [1, 128, 28, 28]          --\n","│    └─Conv2d: 2-9                       [1, 256, 28, 28]          295,168\n","│    └─LeakyReLU: 2-10                   [1, 256, 28, 28]          --\n","│    └─Conv2d: 2-11                      [1, 256, 28, 28]          65,792\n","│    └─LeakyReLU: 2-12                   [1, 256, 28, 28]          --\n","│    └─Conv2d: 2-13                      [1, 512, 28, 28]          1,180,160\n","│    └─LeakyReLU: 2-14                   [1, 512, 28, 28]          --\n","│    └─MaxPool2d: 2-15                   [1, 512, 14, 14]          --\n","├─Sequential: 1-2                        [1, 245]                  --\n","│    └─Flatten: 2-16                     [1, 100352]               --\n","│    └─Linear: 2-17                      [1, 4096]                 411,045,888\n","│    └─BatchNorm1d: 2-18                 [1, 4096]                 8,192\n","│    └─LeakyReLU: 2-19                   [1, 4096]                 --\n","│    └─Linear: 2-20                      [1, 245]                  1,003,765\n","│    └─Sigmoid: 2-21                     [1, 245]                  --\n","==========================================================================================\n","Total params: 413,743,925\n","Trainable params: 413,743,925\n","Non-trainable params: 0\n","Total mult-adds (Units.GIGABYTES): 2.11\n","==========================================================================================\n","Input size (MB): 0.60\n","Forward/backward pass size (MB): 18.53\n","Params size (MB): 1654.98\n","Estimated Total Size (MB): 1674.11\n","=========================================================================================="]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["criterion = YOLOFaceLoss(lambda_coord=hyperparameters['lambda_coord'], lambda_noobj=hyperparameters['lambda_noobj'])\n","optimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])"],"metadata":{"id":"-ltY43deszQ6","executionInfo":{"status":"ok","timestamp":1745060335190,"user_tz":-180,"elapsed":8,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["for epoch in range(hyperparameters['epochs']):\n","    model.train()\n","    total_loss = 0.0\n","    total_obj_loss = 0.0\n","    total_coord_loss = 0.0\n","    total_no_obj_loss = 0.0\n","    total_batches = len(train_loader)\n","    i = 1\n","    for images, labels in train_loader:\n","        if images.size(0) == 1:  # Skip batch if batch size is 1\n","            continue\n","\n","        images = images.to(DEVICE)\n","        labels = [label.to(DEVICE).float() for label in labels]\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","\n","        # Convert labels to a grid format\n","        labels_grid = torch.zeros_like(outputs).to(DEVICE)\n","\n","        for j, label in enumerate(labels):\n","            # print(f\"        {i}\")\n","            if label.numel() > 0:  # Skip empty labels (no faces)\n","                # Extract the coordinates (x, y, w, h) from the label (ignoring class probability)\n","                x, y, w, h = label[0, 1], label[0, 2], label[0, 3], label[0, 4]\n","\n","                # Compute the grid coordinates for the center of the bounding box\n","                center_x, center_y = int(x * hyperparameters['grid_size']), int(y * hyperparameters['grid_size'])\n","\n","                # Compute the grid cell indices covered by the bounding box\n","                # Calculate the bounding box's top-left and bottom-right corners in terms of grid cells\n","                half_w = int(w * hyperparameters['grid_size'] / 2)\n","                half_h = int(h * hyperparameters['grid_size'] / 2)\n","\n","                start_x = max(int((x - w / 2) * hyperparameters['grid_size']), 0)\n","                end_x = min(int((x + w / 2) * hyperparameters['grid_size']), hyperparameters['grid_size'] - 1)\n","                start_y = max(int((y - h / 2) * hyperparameters['grid_size']), 0)\n","                end_y = min(int((y + h / 2) * hyperparameters['grid_size']), hyperparameters['grid_size'] - 1)\n","\n","                # Iterate over the grid cells that the bounding box covers and set them to 1\n","                for gx in range(start_x, end_x + 1):\n","                    for gy in range(start_y, end_y + 1):\n","                        labels_grid[j, gx, gy, 0] = 1  # The first element (index 0) represents the class probability\n","                        labels_grid[j, gx, gy, 1:5] = torch.tensor([x, y, w, h]).to(DEVICE)\n","\n","        loss, coord_loss, obj_loss, no_obj_loss = criterion(outputs, labels_grid)\n","\n","        loss.backward()\n","        optimizer.step()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        total_loss += loss.item()\n","        total_coord_loss += coord_loss.item()\n","        total_obj_loss += obj_loss.item()\n","        total_no_obj_loss += no_obj_loss.item()\n","\n","        print(f\"Batch number: {i} Batch loss: {loss} Total loss: {total_loss} total batches: {len(train_loader)}\")\n","        i = i + 1\n","\n","    print(f\"Epoch {epoch+1}/{hyperparameters['epochs']}, Loss: {total_loss/len(train_loader):.4f}\")\n","\n","    avg_loss = total_loss / total_batches\n","    avg_coord_loss = total_coord_loss / total_batches\n","    avg_obj_loss = total_obj_loss / total_batches\n","    avg_no_obj_loss = total_no_obj_loss / total_batches\n","    model_filename = f\"{hyperparameters['model_name_root']}_{epoch}_{avg_loss:.4f}.pth\"\n","    torch.save(model.state_dict(), model_filename)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"f6bU7xfHs8jg","executionInfo":{"status":"error","timestamp":1745060823670,"user_tz":-180,"elapsed":276438,"user":{"displayName":"Sorin Milutinovici","userId":"06090978200139614225"}},"outputId":"7fef32a3-1a5e-4fae-f978-42e4f32940b4"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch number: 1 Batch loss: 97.94618225097656 Total loss: 97.94618225097656 total batches: 270\n","Batch number: 2 Batch loss: 102.96906280517578 Total loss: 200.91524505615234 total batches: 270\n","Batch number: 3 Batch loss: 77.63569641113281 Total loss: 278.55094146728516 total batches: 270\n","Batch number: 4 Batch loss: 62.48792266845703 Total loss: 341.0388641357422 total batches: 270\n","Batch number: 5 Batch loss: 69.28269958496094 Total loss: 410.3215637207031 total batches: 270\n","Batch number: 6 Batch loss: 52.58104705810547 Total loss: 462.9026107788086 total batches: 270\n","Batch number: 7 Batch loss: 65.20145416259766 Total loss: 528.1040649414062 total batches: 270\n","Batch number: 8 Batch loss: 59.83211135864258 Total loss: 587.9361763000488 total batches: 270\n","Batch number: 9 Batch loss: 37.36440658569336 Total loss: 625.3005828857422 total batches: 270\n","Batch number: 10 Batch loss: 36.92264175415039 Total loss: 662.2232246398926 total batches: 270\n","Batch number: 11 Batch loss: 35.817420959472656 Total loss: 698.0406455993652 total batches: 270\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-568da992a4dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Skip batch if batch size is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-a08c58d2be2a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;31m# Rescale the bounding box coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}